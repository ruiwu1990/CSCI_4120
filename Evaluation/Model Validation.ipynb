{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--BOOK_INFORMATION-->\n",
    "<img align=\"left\" style=\"padding-right:10px;\" src=\"figures/PDSH-cover-small.png\">\n",
    "\n",
    "*This notebook contains an excerpt from the [Python Data Science Handbook](http://shop.oreilly.com/product/0636920034919.do) by Jake VanderPlas; the content is available [on GitHub](https://github.com/jakevdp/PythonDataScienceHandbook).*\n",
    "\n",
    "*The text is released under the [CC-BY-NC-ND license](https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode), and code is released under the [MIT license](https://opensource.org/licenses/MIT). If you find this content useful, please consider supporting the work by [buying the book](http://shop.oreilly.com/product/0636920034919.do)!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics for Evaluation\n",
    "\n",
    "R^2, MSE, RMSE, MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9486081370449679 0.375 0.6123724356957945 0.5\n",
      "0.9382566585956417 [0.41666667 1.        ] [0.5 1. ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from math import sqrt\n",
    "\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "r2_1 = r2_score(y_true, y_pred)\n",
    "mse_1 = mean_squared_error(y_true, y_pred)\n",
    "# sklearn does not have a function to calculate rmse directly\n",
    "rmse_1 = sqrt(mse_1)\n",
    "mae_1 = mean_absolute_error(y_true, y_pred)\n",
    "print(r2_1, mse_1, rmse_1, mae_1)\n",
    "\n",
    "# [n_samples, n_outputs], n_outputs are weights\n",
    "y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
    "y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
    "r2_2 = r2_score(y_true, y_pred,\n",
    "          multioutput='variance_weighted') \n",
    "# \n",
    "mse_2 = mean_squared_error(y_true, y_pred, multioutput='raw_values')  \n",
    "# rmse_2 = sqrt(mse_2)\n",
    "mae_2 = mean_absolute_error(y_true, y_pred, multioutput='raw_values')\n",
    "print(r2_2, mse_2, mae_2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thinking about Model Validation\n",
    "\n",
    "In principle, model validation is very simple: after choosing a model and its hyperparameters, we can estimate how effective it is by applying it to some of the training data and comparing the prediction to the known value.\n",
    "\n",
    "The following sections first show a naive approach to model validation and why it\n",
    "fails, before exploring the use of holdout sets and cross-validation for more robust\n",
    "model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model validation the wrong way\n",
    "\n",
    "Let's demonstrate the naive approach to validation using the Iris data, which we saw in the previous section.\n",
    "We will start by loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we choose a model and hyperparameters. Here we'll use a *k*-neighbors classifier with ``n_neighbors=1``.\n",
    "This is a very simple and intuitive model that says \"the label of an unknown point is the same as the label of its closest training point:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we train the model, and use it to predict labels for data we already know:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)\n",
    "y_model = model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compute the fraction of correctly labeled points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# classification score\n",
    "# In multilabel classification, this function computes subset accuracy: \n",
    "# the set of labels predicted for a sample must exactly match the corresponding set of labels in y_true.\n",
    "accuracy_score(y, y_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see an accuracy score of 1.0, which indicates that 100% of points were correctly labeled by our model!\n",
    "But is this truly measuring the expected accuracy? Have we really come upon a model that we expect to be correct 100% of the time?\n",
    "\n",
    "As you may have gathered, the answer is no.\n",
    "In fact, this approach contains a fundamental flaw: *it trains and evaluates the model on the same data*.\n",
    "Furthermore, the nearest neighbor model is an *instance-based* estimator that simply stores the training data, and predicts labels by comparing new data to these stored points: except in contrived cases, it will get 100% accuracy *every time!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model validation the right way: Holdout sets\n",
    "\n",
    "So what can be done?\n",
    "A better sense of a model's performance can be found using what's known as a *holdout set*: that is, we hold back some subset of the data from the training of the model, and then use this holdout set to check the model performance.\n",
    "This splitting can be done using the ``train_test_split`` utility in Scikit-Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9066666666666666"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# split the data with 50% in each set\n",
    "X1, X2, y1, y2 = train_test_split(X, y, random_state=0,\n",
    "                                  train_size=0.5)\n",
    "\n",
    "# fit the model on one set of data\n",
    "model.fit(X1, y1)\n",
    "\n",
    "# evaluate the model on the second set of data\n",
    "y2_model = model.predict(X2)\n",
    "# Accuracy classification score\n",
    "# In multilabel classification, this function computes subset accuracy:\n",
    "# the set of labels predicted for a sample must exactly match the corresponding set of labels in y_true.\n",
    "accuracy_score(y2, y2_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here a more reasonable result: the nearest-neighbor classifier is about 90% accurate on this hold-out set.\n",
    "The hold-out set is similar to unknown data, because the model has not \"seen\" it before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model validation via cross-validation\n",
    "\n",
    "One disadvantage of using a holdout set for model validation is that we have lost a portion of our data to the model training.\n",
    "In the above case, half the dataset does not contribute to the training of the model!\n",
    "This is not optimal, and can cause problems – especially if the initial set of training data is small.\n",
    "\n",
    "One way to address this is to use *cross-validation*; that is, to do a sequence of fits where each subset of the data is used both as a training set and as a validation set.\n",
    "Visually, it might look something like this:\n",
    "\n",
    "![](figures/05.03-2-fold-CV.png)\n",
    "[figure source in Appendix](06.00-Figure-Code.ipynb#2-Fold-Cross-Validation)\n",
    "\n",
    "Here we do two validation trials, alternately using each half of the data as a holdout set.\n",
    "Using the split data from before, we could implement it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.96, 0.9066666666666666)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2_model = model.fit(X1, y1).predict(X2)\n",
    "y1_model = model.fit(X2, y2).predict(X1)\n",
    "accuracy_score(y1, y1_model), accuracy_score(y2, y2_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What comes out are two accuracy scores, which we could combine (by, say, taking the mean) to get a better measure of the global model performance.\n",
    "This particular form of cross-validation is a *two-fold cross-validation*—that is, one in which we have split the data into two sets and used each in turn as a validation set.\n",
    "\n",
    "We could expand on this idea to use even more trials, and more folds in the data—for example, here is a visual depiction of five-fold cross-validation:\n",
    "\n",
    "![](figures/05.03-5-fold-CV.png)\n",
    "\n",
    "\n",
    "Here we split the data into five groups, and use each of them in turn to evaluate the model fit on the other 4/5 of the data.\n",
    "This would be rather tedious to do by hand, and so we can use Scikit-Learn's ``cross_val_score`` convenience routine to do it succinctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.96666667 0.96666667 0.93333333 0.93333333 1.        ]\n",
      "0.96\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cur_score = cross_val_score(model, X, y, cv=5)\n",
    "print(cur_score)\n",
    "print(cur_score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Activity 6\n",
    "LeaveOneOut. Please complete the todo section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "import numpy as np\n",
    "\n",
    "# initialize our score array\n",
    "score = np.array([])\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "for train_index, test_index in loo.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    # if you want to check how the data looks like\n",
    "    # print(X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    # TODO calculate the score\n",
    "    cur_model_pred = model.fit('''input data''', '''label''').predict('''test data''')\n",
    "    cur_score = accuracy_score('''ground truth''', '''our prediction''')\n",
    "    score = np.insert(score, len(score), '''accuracy score for current loop''')\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have 150 samples, the leave one out cross-validation yields scores for 150 trials, and the score indicates either successful (1.0) or unsuccessful (0.0) prediction.\n",
    "Taking the mean of these gives an estimate of the error rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
